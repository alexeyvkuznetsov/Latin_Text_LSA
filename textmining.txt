https://www.youtube.com/playlist?list=PLTgRMOcmRb3OkdbcwkZKIDPXK1zyg_lm8
https://www.youtube.com/watch?v=_V8eKsto3Ug&t=670s
https://www.youtube.com/watch?v=3rVOSw-wrb4
https://www.youtube.com/watch?v=0gf5iLTbiQM
https://www.youtube.com/user/Simplilearn

https://www.youtube.com/watch?v=JgrSXeVhclc

Sentiment analysis русскоязычных твитов при помощи TensorFlow
https://www.youtube.com/watch?v=CDpbJIbDhys

https://www.youtube.com/watch?v=w_XiXmkCUAg
https://www.youtube.com/watch?v=w_XiXmkCUAg
https://www.youtube.com/watch?v=7YykViYej4E
https://www.youtube.com/watch?v=i7TedqX2Eqo
https://www.youtube.com/watch?v=SlsWlFmOW4Q&list=PLH--szKPas84tedzK0Ow00RPqn6_T2bIF

UDPIPE
https://github.com/sp1158/NeoBanks/blob/16131e3ff881e8e20f57f338103882bf41d9b25f/results/atom/Atom.R

Курс
https://www.asozykin.ru/courses/nnpython
https://www.youtube.com/channel/UC5gufuYHPSsJA-jul-iwyXA



LDA
https://www.rtextminer.com/articles/c_topic_modeling.html
https://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html
!!! Dtm2Docs
https://www.rtextminer.com/reference/Dtm2Docs.html


LSA
https://www.codemiles.com/r-examples/latent-semantic-analysis-for-text-in-r-using-lsa-t11103.html
https://www.rtextminer.com/articles/b_document_clustering.html


Важно
https://www.picconf.ru.net/conf-wos-20
https://na-konferencii.ru/conference/vserossijskaja-nauchno-prakticheskaja-konferencija-jelektronnaja-informacionno-obrazovatelnaja-sreda-vuza-sovremennye-problemy-i-perspektivy-razvitija
http://esp-centr.sfedu.ru/index.php/ru/congreso-2020
https://www.sciencen.org/konferencii/page-33/
https://www.sciencen.org/konferencii/new-docpage-29/



Публикация
https://rpubs.com/about/getting-started
https://rstudio-pubs-static.s3.amazonaws.com/180547_9d1809cb26dd4524a2c710f927e6309c.html
https://yihui.org/knitr/demo/graphics/
https://yihui.org/knitr/demo/manual/
https://bookdown.org/yihui/rmarkdown/

https://habr.com/ru/sandbox/18635/


Tonta, Yaşar & Darvish, Hamid. (2010). Diffusion of latent semantic analysis as a research tool: A social network analysis approach // Journal of Informetrics. 4. 166-174. 10.1016/j.joi.2009.11.003. 

!!! From Frequency to Meaning_ Vector Space Models of Semantics


Скрытый семантический анализ (LSA) может преодолеть проблемы, вызванные использованием статистически полученных концептуальных индексов вместо отдельных слов. Он строит концептуальное векторное пространство, в котором каждый термин или документ представлен в виде вектора в пространстве. Он не только значительно уменьшает размерность, но и обнаруживает важную ассоциативную связь между терминами.

Yu B., Xu Z., Li Ch. (2008). Latent semantic analysis for text categorization using neural network. Knowl.-Based Syst.. 21. 900-904. 10.1016/j.knosys.2008.03.045. 
Yu B., Xu Z., Li Ch. Latent semantic analysis for text categorization using neural network // Knowledge-Based Systems. Volume 21, Issue 8, December 2008, Pages 900-904


Author links open overlay panelBoYuaZong-benXubCheng-huaLic

UDPIPE text2vec
https://stackoverflow.com/questions/50378636/text-similarity-using-pos-tag
https://stackoverflow.com/questions/59602847/text-mining-in-r-with-persian/59613249
https://stackoverflow.com/questions/53806825/creating-a-document-feature-matrix-from-list-of-extracted-phrases-after-using-ph/53823848#53823848
https://stackoverflow.com/questions/53802441/language-based-processing-in-r-selecting-features-in-dfm-with-certain-pointwise/53832908#53832908
https://stackoverflow.com/questions/49464904/how-to-make-words-clustering-in-r-with-udpipe-package
https://coderoad.ru/49464904/%D0%9A%D0%B0%D0%BA-%D1%81%D0%B4%D0%B5%D0%BB%D0%B0%D1%82%D1%8C-words-clustering-%D0%B2-R-%D1%81-%D0%BF%D0%B0%D0%BA%D0%B5%D1%82%D0%BE%D0%BC-udpipe


https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/advancing-text-mining/


install.packages("remotes")
remotes::install_github("kasperwelbers/corpus-tools")

dtm.to.dfm(dtm)





The latent semantic analysis algorithm uses term frequency - inverse document Frequency
(tf-idf) and the concept of linear algebra, such as cosine similarity and Euclidean distance,
to find words with similar meanings. These techniques are a part of distributional
semantics. The other one is word2vec. This is a recent algorithm that has been developed by
Google and can help us find the semantics of words and words that have similar meanings.
We will explore word2vec and other techniques in Chapter 6, Advance Features Engineering
and NLP Algorithms.


In the vector space, words that are closer to one another appear in
similar contexts, and words that are further away from each other are more
dissimilar in respect to the contexts in which they appear. Cosine similarity
is a common method for measuring this. Mathematically, cosine distance
is described as follows:

We intuitively describe cosine similarity as the sum of the product of
all the respective elements of two given vectors, divided by the product of
their Euclidean norms. Two vectors that have a 0-degree difference yield
a cosine similarity of 1; whereas two vectors with a 90-degree difference
yield a cosine similarity of 0. The following is an example of some of the
cosine distances between different word vectors:



Журнал
http://nto-journal.ru/about/

Преобразования
https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html

Метод главных компонент
http://bdemeshev.github.io/r_cycle/cycle_files/06_pca.html

Метод опорных векторов
http://bdemeshev.github.io/r_cycle/cycle_files/11_svm.html

# cast into a Term-Document Matrix
ap_td %>% cast_tdm(term, document, count)

# cast into quanteda's dfm
ap_td %>%
  cast_dfm(term, document, count)

# cast into a Matrix object
m <- ap_td %>%
  cast_sparse(document, term, count)
class(m)

#Construct the LSA model
mylsa <- textmodel_lsa(mydfm)



https://github.com/adsieg/text_similarity/blob/master/EMBEDDING%20(word2vec%2C%20FastText%2C%20Glove%2C%20HomeMadeEmbedding).ipynb

Глубокое обучение на R, тренируем word2vec
https://habr.com/ru/post/258983/
https://www.rdocumentation.org/packages/softmaxreg/versions/1.2
https://gist.github.com/primaryobjects/8038d345aae48ae48988906b0525d175

Cosine Python

https://www.machinelearningplus.com/nlp/cosine-similarity/
https://medium.com/better-programming/introduction-to-gensim-calculating-text-similarity-9e8b55de342d


https://www.rtextminer.com/
https://www.rtextminer.com/articles/c_topic_modeling.html
https://lincolnmullen.com/software/textreuse/
http://text2vec.org/similarity.html
http://ethen8181.github.io/machine-learning/clustering_old/text_similarity/text_similarity.html
https://www.r-bloggers.com/using-cosine-similarity-to-find-matching-documents-a-tutorial-using-senecas-letters-to-his-friend-lucilius/
https://www.machinelearningplus.com/nlp/cosine-similarity/
https://journal.code4lib.org/articles/11626
https://www.markhw.com/blog/tag/Text+Mining
https://fredgibbs.net/tutorials/document-similarity-with-r.html
https://netpeak.net/ru/blog/algoritm-lsa-dlya-poiska-pohozhih-dokumentov/
https://joparga3.github.io/Udemy_text_analysis/index.html

https://github.com/Sathiyarajan/data-science-repo-r-py/blob/9a76df4b28f328e7fb43ecd66876e0d834a6d8ac/R/Mastering-R-Programming/Codes/Section%207/7.3.R
https://github.com/rpwr021/Chicago_Crime_Analysis/blob/8f09b049da3ad31c4b9b61a39a05e552017d68c6/Topic_models.R

https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/advancing-text-mining/
https://quanteda.io/index.html
http://www.scholarpedia.org/article/Latent_semantic_analysis
https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/advancing-text-mining/#unknowncategories



https://simonpaarlberg.com/2012-06-28-latent-semantic-analyses-lsa/

http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/

https://habr.com/ru/post/110078/

http://www.jamesbowman.me/post/semantic-analysis-of-webpages-with-machine-learning-in-go/




https://stackoverflow.com/questions/49464904/how-to-make-words-clustering-in-r-with-udpipe-package



https://www.displayr.com/text-analysis-hooking-up-your-term-document-matrix-to-custom-r-code/

https://medium.com/@adriensieg/text-similarities-da019229c894






